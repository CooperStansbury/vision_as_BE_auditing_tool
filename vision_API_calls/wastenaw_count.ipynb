{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "from google.cloud import vision\n",
    "from google.cloud.vision import types\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import memory_profiler as mem_profile\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "chunk_1 has 1000 files \nchunk_2 has 1000 files \nchunk_3 has 1000 files \nchunk_4 has 1000 files \nchunk_5 has 1000 files \nchunk_6 has 964 files \n"
    }
   ],
   "source": [
    "DIR_PATH = \"/Volumes/Cooper_TB_Drive/research/readmissions/image_files/Washtenaw/\"\n",
    "\n",
    "def get_image_file_chunks(DIR_PATH):\n",
    "    \"\"\"A function to gather image files by chunk.\n",
    "    \n",
    "    Args:\n",
    "        - DIR_PATH (str): the path to the directory.\n",
    "\n",
    "    Returns:\n",
    "        - chunks (dict): keys are chunks, values are lists of image \n",
    "            filepaths for that chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    chunks = {}\n",
    "\n",
    "    for f in os.listdir(DIR_PATH):\n",
    "        chunk = \"_\".join(f.split(\"_\")[:2])\n",
    "        file = f\"{DIR_PATH}{f}\"\n",
    "\n",
    "        if not chunk in chunks.keys():\n",
    "            chunks[chunk] = [file]\n",
    "        else:\n",
    "            chunks[chunk].append(file)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# get Washtenaw chunks\n",
    "chunks = get_image_file_chunks(DIR_PATH)\n",
    "\n",
    "for k, v in chunks.items():\n",
    "    print(f\"{k} has {len(v)} files \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_chunker(arr, chunk_size):\n",
    "    \"\"\" divide an array into chinks of size `chunk_size` \"\"\"\n",
    "    for i in range(0, len(arr), chunk_size):\n",
    "        yield arr[i:i + chunk_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(filepath, features):\n",
    "    \"\"\" A function to gather a response from the Google Vision API\n",
    "\n",
    "    Args: \n",
    "        - filepath (str): a valid image filepath\n",
    "        - features (list of dict): the enums.Feature.Type to include in response,\n",
    "            if available for the given image\n",
    "\n",
    "    Returns:\n",
    "        - response\n",
    "    \"\"\"\n",
    "    with io.open(filepath, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = types.Image(content=content)\n",
    "    return client.annotate_image({'image': image, 'features':features})\n",
    "\n",
    "\n",
    "def response_formatter(response, filepath, county, chunk):\n",
    "    \"\"\"A function to tidy the response for storage in a dataframe.\n",
    "\n",
    "    Args:\n",
    "        - response (response: the API repsonse\n",
    "        - filepath (str): the file name for accounting\n",
    "        - county (str): the county for accounting\n",
    "        - chunk (str): the chunk label for accounting\n",
    "\n",
    "    Returns:\n",
    "        - row (dict): a dictionary for fast conversion to a pd.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # agg labels into a list, single list per image\n",
    "    labels = [label.description for label in response.label_annotations]\n",
    "    confidence = [label.score for label in response.label_annotations]\n",
    "\n",
    "    # get dominant colors by image\n",
    "    dom_colors = response.image_properties_annotation.dominant_colors.colors\n",
    "\n",
    "    rgb = [(int(c.color.red), int(c.color.green), int(c.color.blue)) for c in dom_colors]\n",
    "    pixel_frac = [c.pixel_fraction for c in dom_colors]\n",
    "    color_scores = [c.score for c in dom_colors]\n",
    "\n",
    "    return {'file':filepath,\n",
    "            'county':county,\n",
    "            'labels': labels,\n",
    "            'label_scores': confidence,\n",
    "            'colors':rgb,\n",
    "            'color_pixel_fraction':pixel_frac,\n",
    "            'color_scores':color_scores}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Memory (Before): [197.62890625] MB\nWorking Washtenaw chunk_1.0\nWorking Washtenaw chunk_1.100\nWorking Washtenaw chunk_1.200\nWorking Washtenaw chunk_1.300\nERROR on chunk_1.351: 503 GOAWAY received\nWorking Washtenaw chunk_1.400\nWorking Washtenaw chunk_1.500\nERROR on chunk_1.503: 503 GOAWAY received\nWorking Washtenaw chunk_1.600\nWorking Washtenaw chunk_1.700\nWorking Washtenaw chunk_1.800\nWorking Washtenaw chunk_1.900\nbuilding chunk_1 dataframe...\n(998, 7)\nMemory (After): [318.85546875] MB\nchunk_1 took: 404.13\nMemory (Before): [318.85546875] MB\nWorking Washtenaw chunk_2.0\nWorking Washtenaw chunk_2.100\nWorking Washtenaw chunk_2.200\nWorking Washtenaw chunk_2.300\nWorking Washtenaw chunk_2.400\nWorking Washtenaw chunk_2.500\nWorking Washtenaw chunk_2.600\nWorking Washtenaw chunk_2.700\nWorking Washtenaw chunk_2.800\nWorking Washtenaw chunk_2.900\nbuilding chunk_2 dataframe...\n(1000, 7)\nMemory (After): [269.12109375] MB\nchunk_2 took: 468.75\nMemory (Before): [269.12109375] MB\nWorking Washtenaw chunk_3.0\nWorking Washtenaw chunk_3.100\nWorking Washtenaw chunk_3.200\nWorking Washtenaw chunk_3.300\nWorking Washtenaw chunk_3.400\nERROR on chunk_3.460: 503 GOAWAY received\nWorking Washtenaw chunk_3.500\nWorking Washtenaw chunk_3.600\nWorking Washtenaw chunk_3.700\nWorking Washtenaw chunk_3.800\nWorking Washtenaw chunk_3.900\nbuilding chunk_3 dataframe...\n(999, 7)\nMemory (After): [261.515625] MB\nchunk_3 took: 459.24\nMemory (Before): [261.515625] MB\nWorking Washtenaw chunk_4.0\nWorking Washtenaw chunk_4.100\nWorking Washtenaw chunk_4.200\nERROR on chunk_4.245: 503 GOAWAY received\nWorking Washtenaw chunk_4.300\nWorking Washtenaw chunk_4.400\nWorking Washtenaw chunk_4.500\nWorking Washtenaw chunk_4.600\nWorking Washtenaw chunk_4.700\nERROR on chunk_4.752: 503 GOAWAY received\nWorking Washtenaw chunk_4.800\nWorking Washtenaw chunk_4.900\nbuilding chunk_4 dataframe...\n(998, 7)\nMemory (After): [309.91015625] MB\nchunk_4 took: 479.23\nMemory (Before): [309.91015625] MB\nWorking Washtenaw chunk_5.0\nWorking Washtenaw chunk_5.100\nWorking Washtenaw chunk_5.200\nWorking Washtenaw chunk_5.300\nWorking Washtenaw chunk_5.400\nWorking Washtenaw chunk_5.500\nWorking Washtenaw chunk_5.600\nWorking Washtenaw chunk_5.700\nWorking Washtenaw chunk_5.800\nWorking Washtenaw chunk_5.900\nbuilding chunk_5 dataframe...\n(1000, 7)\nMemory (After): [277.78515625] MB\nchunk_5 took: 496.13\nMemory (Before): [277.78515625] MB\nWorking Washtenaw chunk_6.0\nWorking Washtenaw chunk_6.100\nWorking Washtenaw chunk_6.200\nWorking Washtenaw chunk_6.300\nWorking Washtenaw chunk_6.400\nWorking Washtenaw chunk_6.500\nWorking Washtenaw chunk_6.600\nWorking Washtenaw chunk_6.700\nWorking Washtenaw chunk_6.800\nWorking Washtenaw chunk_6.900\nbuilding chunk_6 dataframe...\n(964, 7)\nMemory (After): [272.60546875] MB\nchunk_6 took: 342.56\nCPU times: user 15min 4s, sys: 38min 36s, total: 53min 40s\nWall time: 1d 20h 10min 4s\n"
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "SUB_CHUNK_SIZE is the number of images per Google Vision API request. \n",
    "\n",
    "The API returns in ~30 seconds, so running all 1000 images in a chunk at once would require ~8 hours.\n",
    "By breaking them up I can monitor status more effectively.\n",
    "\"\"\"\n",
    "\n",
    "COUNTY = 'Washtenaw'\n",
    "DIR_PATH =  f\"/Volumes/Cooper_TB_Drive/research/readmissions/image_files/{COUNTY}/\"\n",
    "SAVEPATH = f\"/Volumes/Cooper_TB_Drive/research/readmissions/google_features/\"\n",
    "SUB_CHUNK_SIZE = 100 # how often to print status messages\n",
    "\n",
    "chunks = get_image_file_chunks(DIR_PATH)\n",
    "client = vision.ImageAnnotatorClient()\n",
    "\n",
    "# the features to include in the response\n",
    "features = [{\"type\": vision.enums.Feature.Type.LABEL_DETECTION},\n",
    "            {\"type\": vision.enums.Feature.Type.IMAGE_PROPERTIES}]\n",
    "\n",
    "# iterate through all chunks of 1000 files\n",
    "for chunk, chunk_files in chunks.items():\n",
    "    # for storing results, per chunk\n",
    "    new_rows = [] \n",
    "\n",
    "    gc.collect()\n",
    "    start_time = time.time()\n",
    "    print(f'Memory (Before): {mem_profile.memory_usage()} MB')\n",
    "\n",
    "    for idx, file in enumerate(chunk_files):\n",
    "\n",
    "        # print status\n",
    "        if idx % SUB_CHUNK_SIZE == 0:\n",
    "            gc.collect()\n",
    "            print(f\"Working {COUNTY} {chunk}.{idx}\")\n",
    "\n",
    "        try:\n",
    "            # the dirty work\n",
    "            response = get_response(file, features)\n",
    "            row = response_formatter(response, file, COUNTY, chunk)\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR on {chunk}.{idx}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        new_rows.append(row)\n",
    "\n",
    "    print(f\"building {chunk} dataframe...\")\n",
    "    df = pd.DataFrame(new_rows)\n",
    "    print(df.shape)\n",
    "    f_name = f\"{SAVEPATH}{COUNTY}_features_{chunk}.csv\"\n",
    "    df.to_csv(f_name, index=False)\n",
    "\n",
    "    print(f'Memory (After): {mem_profile.memory_usage()} MB')\n",
    "    end_time = time.time() - start_time\n",
    "    print(f\"{chunk} took: {end_time/60:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "5959\n5959\n5964\n5\n"
    }
   ],
   "source": [
    "ALL_DIR = f\"/Volumes/Cooper_TB_Drive/research/readmissions/image_files/Washtenaw/\"\n",
    "FEAT_DIR = f\"/Volumes/Cooper_TB_Drive/research/readmissions/google_features/\"\n",
    "\n",
    "all_feats = []\n",
    "\n",
    "for file in os.listdir(FEAT_DIR):\n",
    "    if '.csv' in file:\n",
    "        open_path = f\"{FEAT_DIR}{file}\"\n",
    "\n",
    "        tmp = pd.read_csv(open_path, usecols=['file'])\n",
    "        [all_feats.append(x) for x in tmp['file'].tolist()]\n",
    "\n",
    "print(len(all_feats))\n",
    "print(len(set(all_feats)))\n",
    "\n",
    "recalls = []\n",
    "\n",
    "for file in os.listdir(ALL_DIR):\n",
    "    if '.png' in file:\n",
    "        full_path = f\"{ALL_DIR}{file}\"\n",
    "\n",
    "        if not full_path in all_feats:\n",
    "            recalls.append(full_path)\n",
    "\n",
    "\n",
    "print(len(os.listdir(ALL_DIR)))\n",
    "print(len(recalls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SAVEPATH = f\"/Volumes/Cooper_TB_Drive/research/readmissions/google_features/\"\n",
    "new_rows = []\n",
    "\n",
    "for file in recalls:\n",
    "    response = get_response(file, features)\n",
    "    row = response_formatter(response, file, COUNTY, chunk)\n",
    "    new_rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(new_rows)\n",
    "f_name = f\"{SAVEPATH}Washtenaw_features_RECALLS.csv\"\n",
    "df.to_csv(f_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for file in os.listdir(\"/Volumes/Cooper_TB_Drive/research/readmissions/google_features/\"):\n",
    "#     print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# f_test = '/Volumes/Cooper_TB_Drive/research/readmissions/google_features/Washtenaw_features_chunk_5.csv'\n",
    "\n",
    "# test = pd.read_csv(f_test)\n",
    "# print(test.shape)\n",
    "# test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FEATURE_DIR = '/Volumes/Cooper_TB_Drive/research/readmissions/google_features/' \n",
    "# IMAGE_dir = \"/Volumes/Cooper_TB_Drive/research/readmissions/image_files/Washtenaw/\"\n",
    "\n",
    "# in_feature_results = []\n",
    "\n",
    "\n",
    "# for feature_file in os.listdir(FEATURE_DIR):\n",
    "#     if 'csv' in feature_file:\n",
    "#         f_path = f\"{FEATURE_DIR}{feature_file}\"\n",
    "#         feat_df = pd.read_csv(f_path)\n",
    "\n",
    "#         in_feature_results += feat_df['file'].to_list()\n",
    "\n",
    "# print(len(in_feature_results)\n",
    "# in_feature_results[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}