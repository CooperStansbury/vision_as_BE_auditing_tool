{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "from google.cloud import vision\n",
    "from google.cloud.vision import types\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import memory_profiler as mem_profile\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "chunk_1 has 1000 files \nchunk_2 has 1000 files \nchunk_3 has 1000 files \nchunk_4 has 1000 files \nchunk_5 has 1000 files \nchunk_6 has 1000 files \nchunk_7 has 1000 files \nchunk_8 has 695 files \n"
     ]
    }
   ],
   "source": [
    "DIR_PATH = \"/Volumes/Cooper_TB_Drive/research/readmissions/image_files/Wayne/\"\n",
    "\n",
    "def get_image_file_chunks(DIR_PATH):\n",
    "    \"\"\"A function to gather image files by chunk.\n",
    "    \n",
    "    Args:\n",
    "        - DIR_PATH (str): the path to the directory.\n",
    "\n",
    "    Returns:\n",
    "        - chunks (dict): keys are chunks, values are lists of image \n",
    "            filepaths for that chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    chunks = {}\n",
    "\n",
    "    for f in os.listdir(DIR_PATH):\n",
    "        chunk = \"_\".join(f.split(\"_\")[:2])\n",
    "        file = f\"{DIR_PATH}{f}\"\n",
    "\n",
    "        if not chunk in chunks.keys():\n",
    "            chunks[chunk] = [file]\n",
    "        else:\n",
    "            chunks[chunk].append(file)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# get Washtenaw chunks\n",
    "chunks = get_image_file_chunks(DIR_PATH)\n",
    "\n",
    "for k, v in chunks.items():\n",
    "    print(f\"{k} has {len(v)} files \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_chunker(arr, chunk_size):\n",
    "    \"\"\" divide an array into chinks of size `chunk_size` \"\"\"\n",
    "    for i in range(0, len(arr), chunk_size):\n",
    "        yield arr[i:i + chunk_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(filepath, features):\n",
    "    \"\"\" A function to gather a response from the Google Vision API\n",
    "\n",
    "    Args: \n",
    "        - filepath (str): a valid image filepath\n",
    "        - features (list of dict): the enums.Feature.Type to include in response,\n",
    "            if available for the given image\n",
    "\n",
    "    Returns:\n",
    "        - response\n",
    "    \"\"\"\n",
    "    with io.open(filepath, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = types.Image(content=content)\n",
    "    return client.annotate_image({'image': image, 'features':features})\n",
    "\n",
    "\n",
    "def response_formatter(response, filepath, county, chunk):\n",
    "    \"\"\"A function to tidy the response for storage in a dataframe.\n",
    "\n",
    "    Args:\n",
    "        - response (response: the API repsonse\n",
    "        - filepath (str): the file name for accounting\n",
    "        - county (str): the county for accounting\n",
    "        - chunk (str): the chunk label for accounting\n",
    "\n",
    "    Returns:\n",
    "        - row (dict): a dictionary for fast conversion to a pd.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # agg labels into a list, single list per image\n",
    "    labels = [label.description for label in response.label_annotations]\n",
    "    confidence = [label.score for label in response.label_annotations]\n",
    "\n",
    "    # get dominant colors by image\n",
    "    dom_colors = response.image_properties_annotation.dominant_colors.colors\n",
    "\n",
    "    rgb = [(int(c.color.red), int(c.color.green), int(c.color.blue)) for c in dom_colors]\n",
    "    pixel_frac = [c.pixel_fraction for c in dom_colors]\n",
    "    color_scores = [c.score for c in dom_colors]\n",
    "\n",
    "    return {'file':filepath,\n",
    "            'county':county,\n",
    "            'labels': labels,\n",
    "            'label_scores': confidence,\n",
    "            'colors':rgb,\n",
    "            'color_pixel_fraction':pixel_frac,\n",
    "            'color_scores':color_scores}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Memory (Before): [117.48828125] MB\n",
      "Working Wayne chunk_1.0\n",
      "Working Wayne chunk_1.100\n",
      "Working Wayne chunk_1.200\n",
      "Working Wayne chunk_1.300\n",
      "Working Wayne chunk_1.400\n",
      "Working Wayne chunk_1.500\n",
      "Working Wayne chunk_1.600\n",
      "Working Wayne chunk_1.700\n",
      "ERROR on chunk_1.740: 503 GOAWAY received\n",
      "Working Wayne chunk_1.800\n",
      "Working Wayne chunk_1.900\n",
      "building chunk_1 dataframe...\n",
      "(999, 7)\n",
      "Memory (After): [209.0078125] MB\n",
      "chunk_1 took: 279.79\n",
      "Memory (Before): [209.0078125] MB\n",
      "Working Wayne chunk_2.0\n",
      "Working Wayne chunk_2.100\n",
      "ERROR on chunk_2.190: 503 GOAWAY received\n",
      "Working Wayne chunk_2.200\n",
      "Working Wayne chunk_2.300\n",
      "Working Wayne chunk_2.400\n",
      "Working Wayne chunk_2.500\n",
      "Working Wayne chunk_2.600\n",
      "Working Wayne chunk_2.700\n",
      "Working Wayne chunk_2.800\n",
      "Working Wayne chunk_2.900\n",
      "building chunk_2 dataframe...\n",
      "(999, 7)\n",
      "Memory (After): [209.3359375] MB\n",
      "chunk_2 took: 418.79\n",
      "Memory (Before): [209.3359375] MB\n",
      "Working Wayne chunk_3.0\n",
      "Working Wayne chunk_3.100\n",
      "Working Wayne chunk_3.200\n",
      "Working Wayne chunk_3.300\n",
      "Working Wayne chunk_3.400\n",
      "Working Wayne chunk_3.500\n",
      "ERROR on chunk_3.548: 503 GOAWAY received\n",
      "Working Wayne chunk_3.600\n",
      "Working Wayne chunk_3.700\n",
      "Working Wayne chunk_3.800\n",
      "Working Wayne chunk_3.900\n",
      "ERROR on chunk_3.955: 503 GOAWAY received\n",
      "building chunk_3 dataframe...\n",
      "(998, 7)\n",
      "Memory (After): [208.21484375] MB\n",
      "chunk_3 took: 343.56\n",
      "Memory (Before): [208.21875] MB\n",
      "Working Wayne chunk_4.0\n",
      "Working Wayne chunk_4.100\n",
      "Working Wayne chunk_4.200\n",
      "Working Wayne chunk_4.300\n",
      "Working Wayne chunk_4.400\n",
      "Working Wayne chunk_4.500\n",
      "Working Wayne chunk_4.600\n",
      "Working Wayne chunk_4.700\n",
      "Working Wayne chunk_4.800\n",
      "Working Wayne chunk_4.900\n",
      "building chunk_4 dataframe...\n",
      "(1000, 7)\n",
      "Memory (After): [177.6015625] MB\n",
      "chunk_4 took: 263.11\n",
      "Memory (Before): [177.6015625] MB\n",
      "Working Wayne chunk_5.0\n",
      "Working Wayne chunk_5.100\n",
      "Working Wayne chunk_5.200\n",
      "Working Wayne chunk_5.300\n",
      "Working Wayne chunk_5.400\n",
      "Working Wayne chunk_5.500\n",
      "Working Wayne chunk_5.600\n",
      "Working Wayne chunk_5.700\n",
      "Working Wayne chunk_5.800\n",
      "Working Wayne chunk_5.900\n",
      "building chunk_5 dataframe...\n",
      "(1000, 7)\n",
      "Memory (After): [173.42578125] MB\n",
      "chunk_5 took: 269.42\n",
      "Memory (Before): [173.4296875] MB\n",
      "Working Wayne chunk_6.0\n",
      "Working Wayne chunk_6.100\n",
      "Working Wayne chunk_6.200\n",
      "ERROR on chunk_6.252: 503 GOAWAY received\n",
      "Working Wayne chunk_6.300\n",
      "Working Wayne chunk_6.400\n",
      "Working Wayne chunk_6.500\n",
      "Working Wayne chunk_6.600\n",
      "Working Wayne chunk_6.700\n",
      "Working Wayne chunk_6.800\n",
      "Working Wayne chunk_6.900\n",
      "building chunk_6 dataframe...\n",
      "(999, 7)\n",
      "Memory (After): [189.453125] MB\n",
      "chunk_6 took: 252.87\n",
      "Memory (Before): [189.453125] MB\n",
      "Working Wayne chunk_7.0\n",
      "Working Wayne chunk_7.100\n",
      "Working Wayne chunk_7.200\n",
      "Working Wayne chunk_7.300\n",
      "Working Wayne chunk_7.400\n",
      "Working Wayne chunk_7.500\n",
      "Working Wayne chunk_7.600\n",
      "Working Wayne chunk_7.700\n",
      "Working Wayne chunk_7.800\n",
      "Working Wayne chunk_7.900\n",
      "building chunk_7 dataframe...\n",
      "(1000, 7)\n",
      "Memory (After): [154.66015625] MB\n",
      "chunk_7 took: 209.97\n",
      "Memory (Before): [154.66015625] MB\n",
      "Working Wayne chunk_8.0\n",
      "Working Wayne chunk_8.100\n",
      "Working Wayne chunk_8.200\n",
      "Working Wayne chunk_8.300\n",
      "Working Wayne chunk_8.400\n",
      "Working Wayne chunk_8.500\n",
      "Working Wayne chunk_8.600\n",
      "building chunk_8 dataframe...\n",
      "(695, 7)\n",
      "Memory (After): [167.5625] MB\n",
      "chunk_8 took: 38.75\n",
      "CPU times: user 9min 50s, sys: 25min 13s, total: 35min 3s\n",
      "Wall time: 1d 10h 36min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "SUB_CHUNK_SIZE is the number of images per Google Vision API request. \n",
    "\n",
    "The API returns in ~30 seconds, so running all 1000 images in a chunk at once would require ~8 hours.\n",
    "By breaking them up I can monitor status more effectively.\n",
    "\"\"\"\n",
    "\n",
    "COUNTY = 'Wayne'\n",
    "DIR_PATH =  f\"/Volumes/Cooper_TB_Drive/research/readmissions/image_files/{COUNTY}/\"\n",
    "SAVEPATH = f\"/Volumes/Cooper_TB_Drive/research/readmissions/google_features/\"\n",
    "SUB_CHUNK_SIZE = 100 # how often to print status messages\n",
    "\n",
    "chunks = get_image_file_chunks(DIR_PATH)\n",
    "client = vision.ImageAnnotatorClient()\n",
    "\n",
    "# the features to include in the response\n",
    "features = [{\"type\": vision.enums.Feature.Type.LABEL_DETECTION},\n",
    "            {\"type\": vision.enums.Feature.Type.IMAGE_PROPERTIES}]\n",
    "\n",
    "# iterate through all chunks of 1000 files\n",
    "for chunk, chunk_files in chunks.items():\n",
    "    # for storing results, per chunk\n",
    "    new_rows = [] \n",
    "\n",
    "    gc.collect()\n",
    "    start_time = time.time()\n",
    "    print(f'Memory (Before): {mem_profile.memory_usage()} MB')\n",
    "\n",
    "    for idx, file in enumerate(chunk_files):\n",
    "\n",
    "        # print status\n",
    "        if idx % SUB_CHUNK_SIZE == 0:\n",
    "            gc.collect()\n",
    "            print(f\"Working {COUNTY} {chunk}.{idx}\")\n",
    "\n",
    "        try:\n",
    "            # the dirty work\n",
    "            response = get_response(file, features)\n",
    "            row = response_formatter(response, file, COUNTY, chunk)\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR on {chunk}.{idx}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        new_rows.append(row)\n",
    "\n",
    "    print(f\"building {chunk} dataframe...\")\n",
    "    df = pd.DataFrame(new_rows)\n",
    "    print(df.shape)\n",
    "    f_name = f\"{SAVEPATH}{COUNTY}_features_{chunk}.csv\"\n",
    "    df.to_csv(f_name, index=False)\n",
    "\n",
    "    print(f'Memory (After): {mem_profile.memory_usage()} MB')\n",
    "    end_time = time.time() - start_time\n",
    "    print(f\"{chunk} took: {end_time/60:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "13654\n",
      "13654\n",
      "7695\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "ALL_DIR = f\"/Volumes/Cooper_TB_Drive/research/readmissions/image_files/Wayne/\"\n",
    "FEAT_DIR = f\"/Volumes/Cooper_TB_Drive/research/readmissions/google_features/\"\n",
    "\n",
    "all_feats = []\n",
    "\n",
    "for file in os.listdir(FEAT_DIR):\n",
    "    if '.csv' in file:\n",
    "        open_path = f\"{FEAT_DIR}{file}\"\n",
    "\n",
    "        tmp = pd.read_csv(open_path, usecols=['file'])\n",
    "        [all_feats.append(x) for x in tmp['file'].tolist()]\n",
    "\n",
    "print(len(all_feats))\n",
    "print(len(set(all_feats)))\n",
    "\n",
    "recalls = []\n",
    "\n",
    "for file in os.listdir(ALL_DIR):\n",
    "    if '.png' in file:\n",
    "        full_path = f\"{ALL_DIR}{file}\"\n",
    "\n",
    "        if not full_path in all_feats:\n",
    "            recalls.append(full_path)\n",
    "\n",
    "\n",
    "print(len(os.listdir(ALL_DIR)))\n",
    "print(len(recalls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SAVEPATH = f\"/Volumes/Cooper_TB_Drive/research/readmissions/google_features/\"\n",
    "new_rows = []\n",
    "\n",
    "for file in recalls:\n",
    "    response = get_response(file, features)\n",
    "    row = response_formatter(response, file, COUNTY, chunk)\n",
    "    new_rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(new_rows)\n",
    "f_name = f\"{SAVEPATH}Wayne_features_RECALLS.csv\"\n",
    "df.to_csv(f_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ".DS_Store\nfirst_pass\nWashtenaw_features_chunk_1.csv\nWashtenaw_features_chunk_2.csv\nWashtenaw_features_chunk_3.csv\nWashtenaw_features_chunk_4.csv\nWashtenaw_features_chunk_5.csv\nWashtenaw_features_chunk_6.csv\nWashtenaw_features_RECALLS.csv\nWayne_features_chunk_1.csv\nWayne_features_chunk_2.csv\nWayne_features_chunk_3.csv\nWayne_features_chunk_4.csv\nWayne_features_chunk_5.csv\nWayne_features_chunk_6.csv\nWayne_features_chunk_7.csv\nWayne_features_chunk_8.csv\nWayne_features_RECALLS.csv\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(\"/Volumes/Cooper_TB_Drive/research/readmissions/google_features/\"):\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# f_test = '/Volumes/Cooper_TB_Drive/research/readmissions/google_features/Washtenaw_features_chunk_5.csv'\n",
    "\n",
    "# test = pd.read_csv(f_test)\n",
    "# print(test.shape)\n",
    "# test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FEATURE_DIR = '/Volumes/Cooper_TB_Drive/research/readmissions/google_features/' \n",
    "# IMAGE_dir = \"/Volumes/Cooper_TB_Drive/research/readmissions/image_files/Washtenaw/\"\n",
    "\n",
    "# in_feature_results = []\n",
    "\n",
    "\n",
    "# for feature_file in os.listdir(FEATURE_DIR):\n",
    "#     if 'csv' in feature_file:\n",
    "#         f_path = f\"{FEATURE_DIR}{feature_file}\"\n",
    "#         feat_df = pd.read_csv(f_path)\n",
    "\n",
    "#         in_feature_results += feat_df['file'].to_list()\n",
    "\n",
    "# print(len(in_feature_results)\n",
    "# in_feature_results[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}